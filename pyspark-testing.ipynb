{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f464c216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T22:43:25.218839Z",
     "iopub.status.busy": "2023-10-05T22:43:25.217808Z",
     "iopub.status.idle": "2023-10-05T22:43:56.185539Z",
     "shell.execute_reply": "2023-10-05T22:43:56.184592Z"
    },
    "papermill": {
     "duration": 30.973675,
     "end_time": "2023-10-05T22:43:56.187254",
     "exception": false,
     "start_time": "2023-10-05T22:43:25.213579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425350 sha256=af01d452bc9f299e22ce3f41a111c179503e9cd4b6ca28065ab78a143eb9da2a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032ed596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T22:43:56.199454Z",
     "iopub.status.busy": "2023-10-05T22:43:56.199204Z",
     "iopub.status.idle": "2023-10-05T22:43:56.639853Z",
     "shell.execute_reply": "2023-10-05T22:43:56.639056Z"
    },
    "papermill": {
     "duration": 0.448721,
     "end_time": "2023-10-05T22:43:56.641581",
     "exception": false,
     "start_time": "2023-10-05T22:43:56.192860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b98bc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T22:43:56.653651Z",
     "iopub.status.busy": "2023-10-05T22:43:56.653309Z",
     "iopub.status.idle": "2023-10-05T22:44:01.705700Z",
     "shell.execute_reply": "2023-10-05T22:44:01.705088Z"
    },
    "papermill": {
     "duration": 5.060055,
     "end_time": "2023-10-05T22:44:01.707162",
     "exception": false,
     "start_time": "2023-10-05T22:43:56.647107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/05 22:43:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://44cedf7c6ac4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>testing-NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f90386fc6d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "            builder.\\\n",
    "            master(\"local[2]\").\\\n",
    "            appName(\"testing-NLP\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aa6b1ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T22:44:01.719063Z",
     "iopub.status.busy": "2023-10-05T22:44:01.718807Z",
     "iopub.status.idle": "2023-10-05T22:44:01.724484Z",
     "shell.execute_reply": "2023-10-05T22:44:01.723748Z"
    },
    "papermill": {
     "duration": 0.013218,
     "end_time": "2023-10-05T22:44:01.725997",
     "exception": false,
     "start_time": "2023-10-05T22:44:01.712779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://44cedf7c6ac4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>testing-NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=testing-NLP>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91285f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T22:44:01.738905Z",
     "iopub.status.busy": "2023-10-05T22:44:01.738647Z",
     "iopub.status.idle": "2023-10-05T22:44:06.497238Z",
     "shell.execute_reply": "2023-10-05T22:44:06.496478Z"
    },
    "papermill": {
     "duration": 4.767182,
     "end_time": "2023-10-05T22:44:06.499303",
     "exception": false,
     "start_time": "2023-10-05T22:44:01.732121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|            features|           features2|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|[1.0, 1.0, 1.0, 1.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|\n",
      "|[2.0, 2.0, 2.0, 2.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|\n",
      "|[3.0, 3.0, 3.0, 3.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|\n",
      "+--------------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [([1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0],1.0), \n",
    "        ([2.0, 2.0, 2.0, 2.0],[1.0, 1.0, 1.0, 1.0], 1.0), \n",
    "        ([3.0, 3.0, 3.0, 3.0],[1.0, 1.0, 1.0, 1.0], 1.0)]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"features\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"features2\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"label\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b7965b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T22:44:06.520763Z",
     "iopub.status.busy": "2023-10-05T22:44:06.520265Z",
     "iopub.status.idle": "2023-10-05T22:44:07.159143Z",
     "shell.execute_reply": "2023-10-05T22:44:07.158102Z"
    },
    "papermill": {
     "duration": 0.651409,
     "end_time": "2023-10-05T22:44:07.161003",
     "exception": false,
     "start_time": "2023-10-05T22:44:06.509594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+\n",
      "|            features|           features2|label|         dot_product|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "|[1.0, 1.0, 1.0, 1.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|[1.0, 1.0, 1.0, 1.0]|\n",
      "|[2.0, 2.0, 2.0, 2.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|[2.0, 2.0, 2.0, 2.0]|\n",
      "|[3.0, 3.0, 3.0, 3.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|[3.0, 3.0, 3.0, 3.0]|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dot_product = df.withColumn(\"dot_product\",\n",
    "                               F.expr(\"transform(features, (x, i) -> x * features2[i])\"))\n",
    "df_dot_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c3a204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T22:44:07.182883Z",
     "iopub.status.busy": "2023-10-05T22:44:07.182491Z",
     "iopub.status.idle": "2023-10-05T22:44:07.642251Z",
     "shell.execute_reply": "2023-10-05T22:44:07.641340Z"
    },
    "papermill": {
     "duration": 0.472491,
     "end_time": "2023-10-05T22:44:07.643956",
     "exception": false,
     "start_time": "2023-10-05T22:44:07.171465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+---------------+\n",
      "|            features|           features2|label|         dot_product|dot_product_sum|\n",
      "+--------------------+--------------------+-----+--------------------+---------------+\n",
      "|[1.0, 1.0, 1.0, 1.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|[1.0, 1.0, 1.0, 1.0]|            4.0|\n",
      "|[2.0, 2.0, 2.0, 2.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|[2.0, 2.0, 2.0, 2.0]|            8.0|\n",
      "|[3.0, 3.0, 3.0, 3.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|[3.0, 3.0, 3.0, 3.0]|           12.0|\n",
      "+--------------------+--------------------+-----+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sum of the array elements and create a new column\n",
    "df_dot_product = df_dot_product.withColumn(\"dot_product_sum\",\n",
    "                                           F.expr('aggregate(dot_product, 0D, (acc, x) -> acc + x)'))\n",
    "df_dot_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "586e9a63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T22:44:07.663768Z",
     "iopub.status.busy": "2023-10-05T22:44:07.663480Z",
     "iopub.status.idle": "2023-10-05T22:44:08.147191Z",
     "shell.execute_reply": "2023-10-05T22:44:08.146429Z"
    },
    "papermill": {
     "duration": 0.49597,
     "end_time": "2023-10-05T22:44:08.149194",
     "exception": false,
     "start_time": "2023-10-05T22:44:07.653224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+---------------+--------------------+--------------------+\n",
      "|            features|           features2|label|         dot_product|dot_product_sum|            mag_list|           mag_list2|\n",
      "+--------------------+--------------------+-----+--------------------+---------------+--------------------+--------------------+\n",
      "|[1.0, 1.0, 1.0, 1.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|[1.0, 1.0, 1.0, 1.0]|            4.0|[1.0, 1.0, 1.0, 1.0]|[1.0, 1.0, 1.0, 1.0]|\n",
      "|[2.0, 2.0, 2.0, 2.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|[2.0, 2.0, 2.0, 2.0]|            8.0|[4.0, 4.0, 4.0, 4.0]|[1.0, 1.0, 1.0, 1.0]|\n",
      "|[3.0, 3.0, 3.0, 3.0]|[1.0, 1.0, 1.0, 1.0]|  1.0|[3.0, 3.0, 3.0, 3.0]|           12.0|[9.0, 9.0, 9.0, 9.0]|[1.0, 1.0, 1.0, 1.0]|\n",
      "+--------------------+--------------------+-----+--------------------+---------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the magnitude of each vector\n",
    "df_dot_product = df_dot_product.withColumn(\"mag_list\", \n",
    "                    F.expr(\"transform(features, x -> x * x)\"))\n",
    "# Calculate the magnitude of each vector\n",
    "df_dot_product = df_dot_product.withColumn(\"mag_list2\", \n",
    "                    F.expr(\"transform(features2, x -> x * x)\"))\n",
    "df_dot_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13cc289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-05T22:44:08.170065Z",
     "iopub.status.busy": "2023-10-05T22:44:08.169716Z",
     "iopub.status.idle": "2023-10-05T22:44:08.856975Z",
     "shell.execute_reply": "2023-10-05T22:44:08.856088Z"
    },
    "papermill": {
     "duration": 0.699355,
     "end_time": "2023-10-05T22:44:08.858863",
     "exception": false,
     "start_time": "2023-10-05T22:44:08.159508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+------------+-------------+\n",
      "|            features|           features2|         dot_product|dot_product_sum|            mag_list|           mag_list2|mag_list_sum|mag_list_sum2|\n",
      "+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+------------+-------------+\n",
      "|[1.0, 1.0, 1.0, 1.0]|[1.0, 1.0, 1.0, 1.0]|[1.0, 1.0, 1.0, 1.0]|            4.0|[1.0, 1.0, 1.0, 1.0]|[1.0, 1.0, 1.0, 1.0]|         2.0|          2.0|\n",
      "|[2.0, 2.0, 2.0, 2.0]|[1.0, 1.0, 1.0, 1.0]|[2.0, 2.0, 2.0, 2.0]|            8.0|[4.0, 4.0, 4.0, 4.0]|[1.0, 1.0, 1.0, 1.0]|         4.0|          2.0|\n",
      "|[3.0, 3.0, 3.0, 3.0]|[1.0, 1.0, 1.0, 1.0]|[3.0, 3.0, 3.0, 3.0]|           12.0|[9.0, 9.0, 9.0, 9.0]|[1.0, 1.0, 1.0, 1.0]|         6.0|          2.0|\n",
      "+--------------------+--------------------+--------------------+---------------+--------------------+--------------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dot_product = df_dot_product.drop('label') \n",
    "df_dot_product = df_dot_product.drop('label') \n",
    "df_dot_product = df_dot_product.withColumn(\"mag_list_sum\",\n",
    "                    F.sqrt(F.expr('aggregate(mag_list, 0D, (acc, x) -> acc + x)')))\n",
    "df_dot_product = df_dot_product.withColumn(\"mag_list_sum2\",\n",
    "                    F.sqrt(F.expr('aggregate(mag_list2, 0D, (acc, x) -> acc + x)')))\n",
    "df_dot_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb959a",
   "metadata": {
    "papermill": {
     "duration": 0.009958,
     "end_time": "2023-10-05T22:44:08.878728",
     "exception": false,
     "start_time": "2023-10-05T22:44:08.868770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4116789a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-05T22:44:08.901068Z",
     "iopub.status.busy": "2023-10-05T22:44:08.900537Z",
     "iopub.status.idle": "2023-10-05T22:44:08.907169Z",
     "shell.execute_reply": "2023-10-05T22:44:08.906248Z"
    },
    "papermill": {
     "duration": 0.020016,
     "end_time": "2023-10-05T22:44:08.909255",
     "exception": false,
     "start_time": "2023-10-05T22:44:08.889239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 48.703344,
   "end_time": "2023-10-05T22:44:11.539047",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-05T22:43:22.835703",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
